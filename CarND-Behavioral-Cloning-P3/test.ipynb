{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "#import pydot2\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import Convolution2D, Input, Dropout\n",
    "from keras.layers import Flatten, Dense\n",
    "from keras.models import Model\n",
    "#from keras.utils.visualize_util import plot\n",
    "#from keras.utils import plot_model\n",
    "#from utils import RegressionImageDataGenerator, get_cropped_shape, load_images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = [160, 320]\n",
    "CROPPING = (54, 0, 0, 0)\n",
    "SHIFT_OFFSET = 0.2\n",
    "SHIFT_RANGE = 0.2\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PATIENCE = 3\n",
    "NB_EPOCH = 50\n",
    "\n",
    "TRAINING_DATA_PATHS = ['/Users/dc/Downloads/data/track1_central/driving_log.csv',\n",
    "                       '/Users/dc/Downloads/data/track1_recovery/driving_log.csv',\n",
    "                       '/Users/dc/Downloads/data/track1_reverse/driving_log.csv',\n",
    "                       '/Users/dc/Downloads/data/track1_recovery_reverse/driving_log.csv',\n",
    "                       '/Users/dc/Downloads/data/track2_central/driving_log.csv']\n",
    "\n",
    "VALIDATION_DATA_PATHS = ['/Users/dc/Downloads/data/track1_test/driving_log.csv',\n",
    "                         '/Users/dc/Downloads/data/track2_test/driving_log.csv']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc as spm\n",
    "from keras.preprocessing.image import *\n",
    "\n",
    "\n",
    "def normalize(images, new_max, new_min, old_max=None, old_min=None):\n",
    "    if old_min is None:\n",
    "        old_min = np.min(images)\n",
    "    if old_max is None:\n",
    "        old_max = np.max(images)\n",
    "\n",
    "    return (images - old_min) * ((new_max - new_min) / (old_max - old_min)) + new_min\n",
    "\n",
    "\n",
    "def crop_image(img, cropping):\n",
    "    return img[cropping[0]:img.shape[0] - cropping[1], cropping[2]:img.shape[1] - cropping[3], :]\n",
    "\n",
    "\n",
    "def get_cropped_shape(img_shape, cropping):\n",
    "    return (img_shape[0] - cropping[0] - cropping[1],\n",
    "            img_shape[1] - cropping[2] - cropping[3],\n",
    "            img_shape[2])\n",
    "\n",
    "\n",
    "def resize_image(img, size):\n",
    "    return spm.imresize(img, size)\n",
    "\n",
    "\n",
    "def extract_filename(path):\n",
    "    return path.split('/')[-1]\n",
    "\n",
    "\n",
    "def adjust_path(path, new_location):\n",
    "    return '%s/%s' % (new_location, extract_filename(path))\n",
    "\n",
    "\n",
    "def load_images(paths, target_size):\n",
    "    images = np.zeros((len(paths), *target_size, 3))\n",
    "    for i, p in enumerate(paths):\n",
    "        img = load_img(p, target_size=target_size)\n",
    "        img = img_to_array(img, dim_ordering='tf')\n",
    "        images[i] = img\n",
    "\n",
    "    return images\n",
    "\n",
    "\n",
    "class RegressionImageDataGenerator(object):\n",
    "    \"\"\"Generate minibatches with\n",
    "    real-time data augmentation.\n",
    "    This implementation is a modified version of the ImageDataGenerator from Keras\n",
    "    (https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py).\n",
    "    # Arguments\n",
    "        featurewise_center: set input mean to 0 over the dataset.\n",
    "        samplewise_center: set each sample mean to 0.\n",
    "        featurewise_std_normalization: divide inputs by std of the dataset.\n",
    "        samplewise_std_normalization: divide each input by its std.\n",
    "        zca_whitening: apply ZCA whitening.\n",
    "        rotation_range: degrees (0 to 180).\n",
    "        rotation_value_transform: function to modify the label based on the rotation.\n",
    "        width_shift_range: fraction of total width.\n",
    "        width_shift_value_transform: function to modify the label based on the width_shift.\n",
    "        height_shift_range: fraction of total height.\n",
    "        height_shift_value_transform: function to modify the label based on the height_shift.\n",
    "        shear_range: shear intensity (shear angle in radians).\n",
    "        shear_value_transform: function to modify the label based on the shear.\n",
    "        zoom_range: amount of zoom. if scalar z, zoom will be randomly picked\n",
    "            in the range [1-z, 1+z]. A sequence of two can be passed instead\n",
    "            to select this range.\n",
    "        zoom_value_transform: function to modify the label based on the zoom.\n",
    "        channel_shift_range: shift range for each channels.\n",
    "        fill_mode: points outside the boundaries are filled according to the\n",
    "            given mode ('constant', 'nearest', 'reflect' or 'wrap'). Default\n",
    "            is 'nearest'.\n",
    "        cval: value used for points outside the boundaries when fill_mode is\n",
    "            'constant'. Default is 0.\n",
    "        horizontal_flip: whether to randomly flip images horizontally.\n",
    "        horizontal_flip_value_transform: function to modify the label based on the horizontal_flip.\n",
    "        vertical_flip: whether to randomly flip images vertically.\n",
    "        vertical_flip_value_transform: function to modify the label based on the vertical_flip.\n",
    "        rescale: rescaling factor. If None or 0, no rescaling is applied,\n",
    "            otherwise we multiply the data by the value provided (before applying\n",
    "            any other transformation).\n",
    "        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n",
    "            (the depth) is at index 1, in 'tf' mode it is at index 3.\n",
    "            It defaults to the `image_dim_ordering` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be \"th\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 featurewise_center=False,\n",
    "                 samplewise_center=False,\n",
    "                 featurewise_std_normalization=False,\n",
    "                 samplewise_std_normalization=False,\n",
    "                 zca_whitening=False,\n",
    "                 rotation_range=0.,\n",
    "                 rotation_value_transform=None,\n",
    "                 width_shift_range=0.,\n",
    "                 width_shift_value_transform=None,\n",
    "                 height_shift_range=0.,\n",
    "                 height_shift_value_transform=None,\n",
    "                 shear_range=0.,\n",
    "                 shear_value_transform=None,\n",
    "                 zoom_range=0.,\n",
    "                 zoom_value_transform=None,\n",
    "                 channel_shift_range=0.,\n",
    "                 fill_mode='nearest',\n",
    "                 cval=0.,\n",
    "                 horizontal_flip=False,\n",
    "                 horizontal_flip_value_transform=None,\n",
    "                 vertical_flip=False,\n",
    "                 vertical_flip_value_transform=None,\n",
    "                 rescale=None,\n",
    "                 dim_ordering='default',\n",
    "                 cropping=(0, 0, 0, 0)):\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.__dict__.update(locals())\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "        self.principal_components = None\n",
    "        self.rescale = rescale\n",
    "\n",
    "        if dim_ordering not in {'tf', 'th'}:\n",
    "            raise Exception('dim_ordering should be \"tf\" (channel after row and '\n",
    "                            'column) or \"th\" (channel before row and column). '\n",
    "                            'Received arg: ', dim_ordering)\n",
    "        self.dim_ordering = dim_ordering\n",
    "        if dim_ordering == 'th':\n",
    "            self.channel_index = 1\n",
    "            self.row_index = 2\n",
    "            self.col_index = 3\n",
    "        if dim_ordering == 'tf':\n",
    "            self.channel_index = 3\n",
    "            self.row_index = 1\n",
    "            self.col_index = 2\n",
    "\n",
    "        if np.isscalar(zoom_range):\n",
    "            self.zoom_range = [1 - zoom_range, 1 + zoom_range]\n",
    "        elif len(zoom_range) == 2:\n",
    "            self.zoom_range = [zoom_range[0], zoom_range[1]]\n",
    "        else:\n",
    "            raise Exception('zoom_range should be a float or '\n",
    "                            'a tuple or list of two floats. '\n",
    "                            'Received arg: ', zoom_range)\n",
    "\n",
    "        self.cropping = cropping\n",
    "\n",
    "    def flow(self, X, y=None, batch_size=32, shuffle=True, seed=None,\n",
    "             save_to_dir=None, save_prefix='', save_format='jpeg'):\n",
    "        return RegressionNumpyArrayIterator(\n",
    "            X, y, self,\n",
    "            batch_size=batch_size, shuffle=shuffle, seed=seed,\n",
    "            dim_ordering=self.dim_ordering,\n",
    "            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n",
    "\n",
    "    def flow_from_directory(self, directory, values,\n",
    "                            target_size=(256, 256), color_mode='rgb',\n",
    "                            batch_size=32, shuffle=True, seed=None,\n",
    "                            save_to_dir=None, save_prefix='', save_format='jpeg'):\n",
    "        return RegressionDirectoryIterator(\n",
    "            directory, values, self,\n",
    "            target_size=target_size, color_mode=color_mode,\n",
    "            dim_ordering=self.dim_ordering,\n",
    "            batch_size=batch_size, shuffle=shuffle, seed=seed,\n",
    "            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n",
    "\n",
    "    def crop(self, x):\n",
    "        return crop_image(x, self.cropping)\n",
    "\n",
    "    def standardize(self, x):\n",
    "        if self.rescale:\n",
    "            if callable(self.rescale):\n",
    "                x = self.rescale(x)\n",
    "            else:\n",
    "                x *= self.rescale\n",
    "\n",
    "        # x is a single image, so it doesn't have image number at index 0\n",
    "        img_channel_index = self.channel_index - 1\n",
    "        if self.samplewise_center:\n",
    "            x -= np.mean(x, axis=img_channel_index, keepdims=True)\n",
    "        if self.samplewise_std_normalization:\n",
    "            x /= (np.std(x, axis=img_channel_index, keepdims=True) + 1e-7)\n",
    "\n",
    "        if self.featurewise_center:\n",
    "            x -= self.mean\n",
    "        if self.featurewise_std_normalization:\n",
    "            x /= (self.std + 1e-7)\n",
    "\n",
    "        if self.zca_whitening:\n",
    "            flatx = np.reshape(x, (x.size))\n",
    "            whitex = np.dot(flatx, self.principal_components)\n",
    "            x = np.reshape(whitex, (x.shape[0], x.shape[1], x.shape[2]))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def random_transform(self, x, y):\n",
    "        # x is a single image, so it doesn't have image number at index 0\n",
    "        img_row_index = self.row_index - 1\n",
    "        img_col_index = self.col_index - 1\n",
    "        img_channel_index = self.channel_index - 1\n",
    "\n",
    "        # use composition of homographies to generate final transform that needs to be applied\n",
    "        if self.rotation_range:\n",
    "            theta = np.pi / 180 * np.random.uniform(-self.rotation_range, self.rotation_range)\n",
    "        else:\n",
    "            theta = 0\n",
    "        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                    [np.sin(theta), np.cos(theta), 0],\n",
    "                                    [0, 0, 1]])\n",
    "        if self.rotation_value_transform:\n",
    "            y = self.rotation_value_transform(y, theta)\n",
    "\n",
    "        if self.height_shift_range:\n",
    "            px = np.random.uniform(-self.height_shift_range, self.height_shift_range)\n",
    "            tx = px * x.shape[img_row_index]\n",
    "        else:\n",
    "            tx = 0\n",
    "\n",
    "        if self.height_shift_value_transform:\n",
    "            y = self.height_shift_value_transform(y, px)\n",
    "\n",
    "        if self.width_shift_range:\n",
    "            py = np.random.uniform(-self.width_shift_range, self.width_shift_range)\n",
    "            ty = py * x.shape[img_col_index]\n",
    "        else:\n",
    "            ty = 0\n",
    "\n",
    "        if self.width_shift_value_transform:\n",
    "            y = self.width_shift_value_transform(y, py)\n",
    "\n",
    "        translation_matrix = np.array([[1, 0, tx],\n",
    "                                       [0, 1, ty],\n",
    "                                       [0, 0, 1]])\n",
    "        if self.shear_range:\n",
    "            shear = np.random.uniform(-self.shear_range, self.shear_range)\n",
    "        else:\n",
    "            shear = 0\n",
    "        shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
    "                                 [0, np.cos(shear), 0],\n",
    "                                 [0, 0, 1]])\n",
    "\n",
    "        if self.shear_value_transform:\n",
    "            y = self.shear_value_transform(y, shear)\n",
    "\n",
    "        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n",
    "            zx, zy = 1, 1\n",
    "        else:\n",
    "            zx, zy = np.random.uniform(self.zoom_range[0], self.zoom_range[1], 2)\n",
    "        zoom_matrix = np.array([[zx, 0, 0],\n",
    "                                [0, zy, 0],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "        if self.zoom_value_transform:\n",
    "            y = self.zoom_value_transform(y, zx, zy)\n",
    "\n",
    "        transform_matrix = np.dot(np.dot(np.dot(rotation_matrix, translation_matrix), shear_matrix), zoom_matrix)\n",
    "\n",
    "        h, w = x.shape[img_row_index], x.shape[img_col_index]\n",
    "        transform_matrix = transform_matrix_offset_center(transform_matrix, h, w)\n",
    "        x = apply_transform(x, transform_matrix, img_channel_index,\n",
    "                            fill_mode=self.fill_mode, cval=self.cval)\n",
    "        if self.channel_shift_range != 0:\n",
    "            x = random_channel_shift(x, self.channel_shift_range, img_channel_index)\n",
    "\n",
    "        if self.horizontal_flip:\n",
    "            if np.random.random() < 0.5:\n",
    "                x = flip_axis(x, img_col_index)\n",
    "                if self.horizontal_flip_value_transform:\n",
    "                    y = self.horizontal_flip_value_transform(y)\n",
    "\n",
    "        if self.vertical_flip:\n",
    "            if np.random.random() < 0.5:\n",
    "                x = flip_axis(x, img_row_index)\n",
    "                if self.vertical_flip_value_transform:\n",
    "                    y = self.vertical_flip_value_transform(y)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def fit(self, X,\n",
    "            augment=False,\n",
    "            rounds=1,\n",
    "            seed=None):\n",
    "        '''Required for featurewise_center, featurewise_std_normalization\n",
    "        and zca_whitening.\n",
    "        # Arguments\n",
    "            X: Numpy array, the data to fit on.\n",
    "            augment: whether to fit on randomly augmented samples\n",
    "            rounds: if `augment`,\n",
    "                how many augmentation passes to do over the data\n",
    "            seed: random seed.\n",
    "        '''\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        X = np.copy(X)\n",
    "        cropped = np.zeros((X.shape[0], *get_cropped_shape(X.shape[1:], self.cropping)))\n",
    "        for i, img in enumerate(X):\n",
    "            cropped[i] = self.crop(img)\n",
    "        X = cropped\n",
    "\n",
    "        if augment:\n",
    "            aX = np.zeros(tuple([rounds * X.shape[0]] + list(X.shape)[1:]))\n",
    "            for r in range(rounds):\n",
    "                for i in range(X.shape[0]):\n",
    "                    aX[i + r * X.shape[0]] = self.random_transform(X[i])\n",
    "            X = aX\n",
    "\n",
    "        if self.featurewise_center:\n",
    "            self.mean = np.mean(X, axis=0)\n",
    "            X -= self.mean\n",
    "\n",
    "        if self.featurewise_std_normalization:\n",
    "            self.std = np.std(X, axis=0)\n",
    "            X /= (self.std + 1e-7)\n",
    "\n",
    "        if self.zca_whitening:\n",
    "            flatX = np.reshape(X, (X.shape[0], X.shape[1] * X.shape[2] * X.shape[3]))\n",
    "            sigma = np.dot(flatX.T, flatX) / flatX.shape[0]\n",
    "            U, S, V = linalg.svd(sigma)\n",
    "            self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n",
    "\n",
    "\n",
    "class RegressionNumpyArrayIterator(Iterator):\n",
    "    \"\"\"\n",
    "        This implementation is a modified version of the NumpyArrayIterator from Keras\n",
    "        (https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, image_data_generator,\n",
    "                 batch_size=32, shuffle=False, seed=None,\n",
    "                 dim_ordering='default',\n",
    "                 save_to_dir=None, save_prefix='', save_format='jpeg'):\n",
    "        if y is not None and len(X) != len(y):\n",
    "            raise Exception('X (images tensor) and y (labels) '\n",
    "                            'should have the same length. '\n",
    "                            'Found: X.shape = %s, y.shape = %s' % (np.asarray(X).shape, np.asarray(y).shape))\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.dim_ordering = dim_ordering\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "        super(RegressionNumpyArrayIterator, self).__init__(X.shape[0], batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        # for python 2.x.\n",
    "        # Keeps under lock only the mechanism which advances\n",
    "        # the indexing of each batch\n",
    "        # see http://anandology.com/blog/using-iterators-and-generators/\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock so it can be done in parallel\n",
    "        output_shape = get_cropped_shape(self.X[0].shape, self.image_data_generator.cropping)\n",
    "        batch_x = np.zeros((current_batch_size,) + output_shape)\n",
    "        batch_y = np.zeros(current_batch_size)\n",
    "        for i, j in enumerate(index_array):\n",
    "            x = self.X[j]\n",
    "            y = self.y[j]\n",
    "            x = self.image_data_generator.crop(x)\n",
    "            x, y = self.image_data_generator.random_transform(x, y)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "        if self.save_to_dir:\n",
    "            for i in range(current_batch_size):\n",
    "                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=current_index + i,\n",
    "                                                                  hash=np.random.randint(1e4),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        return batch_x, batch_y\n",
    "\n",
    "\n",
    "class RegressionDirectoryIterator(Iterator):\n",
    "    \"\"\"\n",
    "        This implementation is a modified version of the DirectoryIterator from Keras\n",
    "        (https://github.com/fchollet/keras/blob/master/keras/preprocessing/image.py).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, paths, values, image_data_generator,\n",
    "                 target_size=(256, 256), color_mode='rgb',\n",
    "                 dim_ordering='default',\n",
    "                 batch_size=32, shuffle=True, seed=None,\n",
    "                 save_to_dir=None, save_prefix='', save_format='jpeg'):\n",
    "\n",
    "        if dim_ordering == 'default':\n",
    "            dim_ordering = K.image_dim_ordering()\n",
    "        self.paths = paths\n",
    "        self.values = values\n",
    "        self.image_data_generator = image_data_generator\n",
    "        self.target_size = tuple(target_size)\n",
    "\n",
    "        if color_mode not in {'rgb', 'grayscale'}:\n",
    "            raise ValueError('Invalid color mode:', color_mode,\n",
    "                             '; expected \"rgb\" or \"grayscale\".')\n",
    "\n",
    "        self.color_mode = color_mode\n",
    "        self.dim_ordering = dim_ordering\n",
    "\n",
    "        if self.color_mode == 'rgb':\n",
    "            if self.dim_ordering == 'tf':\n",
    "                self.image_shape = self.target_size + (3,)\n",
    "            else:\n",
    "                self.image_shape = (3,) + self.target_size\n",
    "        else:\n",
    "            if self.dim_ordering == 'tf':\n",
    "                self.image_shape = self.target_size + (1,)\n",
    "            else:\n",
    "                self.image_shape = (1,) + self.target_size\n",
    "\n",
    "        self.save_to_dir = save_to_dir\n",
    "        self.save_prefix = save_prefix\n",
    "        self.save_format = save_format\n",
    "\n",
    "        self.nb_sample = len(paths)\n",
    "        self.nb_values = len(values)\n",
    "        if self.nb_sample != self.nb_values:\n",
    "            raise ValueError(\"Number of values=%d does not match \"\n",
    "                             \"number of samples=%d\" % (self.nb_values, self.nb_sample))\n",
    "\n",
    "        super(RegressionDirectoryIterator, self).__init__(self.nb_sample, batch_size, shuffle, seed)\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        # The transformation of images is not under thread lock so it can be done in parallel\n",
    "        output_shape = get_cropped_shape(self.image_shape, self.image_data_generator.cropping)\n",
    "        batch_x = np.zeros((current_batch_size,) + output_shape)\n",
    "        batch_y = np.zeros(current_batch_size)\n",
    "        grayscale = self.color_mode == 'grayscale'\n",
    "\n",
    "        # build batch of image data\n",
    "        for i, j in enumerate(index_array):\n",
    "            path = self.paths[j]\n",
    "            img = load_img(path, grayscale=grayscale, target_size=self.target_size)\n",
    "\n",
    "            y = self.values[j]\n",
    "            x = img_to_array(img, dim_ordering=self.dim_ordering)\n",
    "            x = self.image_data_generator.crop(x)\n",
    "            x, y = self.image_data_generator.random_transform(x, y)\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "\n",
    "            batch_x[i] = x\n",
    "            batch_y[i] = y\n",
    "\n",
    "        # optionally save augmented images to disk for debugging purposes\n",
    "        if self.save_to_dir:\n",
    "            for i in range(current_batch_size):\n",
    "                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=current_index + i,\n",
    "                                                                  hash=np.random.randint(1e4),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 106, 320, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv1 (Convolution2D)     (None, 106, 320, 64)  1792        input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "block1_conv2 (Convolution2D)     (None, 106, 320, 64)  36928       block1_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)       (None, 53, 160, 64)   0           block1_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv1 (Convolution2D)     (None, 53, 160, 128)  73856       block1_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block2_conv2 (Convolution2D)     (None, 53, 160, 128)  147584      block2_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)       (None, 26, 80, 128)   0           block2_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv1 (Convolution2D)     (None, 26, 80, 256)   295168      block2_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv2 (Convolution2D)     (None, 26, 80, 256)   590080      block3_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_conv3 (Convolution2D)     (None, 26, 80, 256)   590080      block3_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)       (None, 13, 40, 256)   0           block3_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv1 (Convolution2D)     (None, 13, 40, 512)   1180160     block3_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv2 (Convolution2D)     (None, 13, 40, 512)   2359808     block4_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_conv3 (Convolution2D)     (None, 13, 40, 512)   2359808     block4_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)       (None, 6, 20, 512)    0           block4_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv1 (Convolution2D)     (None, 3, 10, 512)    2359808     block4_pool[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv2 (Convolution2D)     (None, 3, 5, 512)     2359808     block5_conv1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "block5_conv3 (Convolution2D)     (None, 3, 3, 512)     2359808     block5_conv2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 4608)          0           block5_conv3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)              (None, 4608)          0           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 2048)          9439232     dropout_4[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)              (None, 2048)          0           fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "fc2 (Dense)                      (None, 1024)          2098176     dropout_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)              (None, 1024)          0           fc2[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "predictions (Dense)              (None, 1)             1025        dropout_6[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 26,253,121\n",
      "Trainable params: 18,617,857\n",
      "Non-trainable params: 7,635,264\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/threading.py\", line 862, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\", line 429, in data_generator_task\n",
      "    generator_output = next(self._generator)\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/preprocessing/image.py\", line 662, in __next__\n",
      "    return self.next(*args, **kwargs)\n",
      "  File \"<ipython-input-3-9a120aab8398>\", line 440, in next\n",
      "    img = load_img(path, grayscale=grayscale, target_size=self.target_size)\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/preprocessing/image.py\", line 296, in load_img\n",
      "    img = pil_image.open(path)\n",
      "  File \"/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/PIL/Image.py\", line 2312, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/home/patrick/Dev/SDCND/Term1/Lesson13 - Behavioral Cloning/data/track1_central/IMG/center_2016_11_22_15_00_11_651.jpg'\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unorderable types: int() < RegressionDirectoryIterator()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-96055b10fd5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                                   \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrdi_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                                   \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNB_EPOCH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                                   callbacks=[checkpoint, early_stopping])\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# summarize history for loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dc/anaconda/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0msamples_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1520\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0msamples_seen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msamples_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m                     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m                     \u001b[0;32mwhile\u001b[0m \u001b[0menqueuer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unorderable types: int() < RegressionDirectoryIterator()"
     ]
    }
   ],
   "source": [
    "def get_generator(train_paths, val_paths, from_directory=False, batch_size=32, fit_sample_size=None):\n",
    "    \"\"\"\n",
    "    Creates an image data generator for traning data and one for validation data. Left and Right images are\n",
    "    added with an offset steering angle.\n",
    "    :param train_paths: A list of paths to all the log files used for training\n",
    "    :param val_paths: A list of paths to all the log files used for validation\n",
    "    :param from_directory: When true all images will be loaded from disk during training\n",
    "    :param batch_size: The batch size to use\n",
    "    :param fit_sample_size: The sample size used to fit the image generator. If set to None, no fit will be done.\n",
    "    :return: training generator, validation generator\n",
    "    \"\"\"\n",
    "    header = ['center_img', 'left_img', 'right_img', 'steering_angle', 'throttle', 'break', 'speed']\n",
    "\n",
    "    log = pd.concat([pd.read_csv(path, names=header) for path in train_paths])\n",
    "    val_log = pd.concat([pd.read_csv(path, names=header) for path in val_paths])\n",
    "\n",
    "    # Create feature value pairs for the left camera images by subtracting the offset from the steering angle.\n",
    "    log_left = log[['left_img', 'steering_angle']].copy()\n",
    "    log_left.loc[:, 'steering_angle'] -= SHIFT_OFFSET\n",
    "\n",
    "    # Create feature value pairs for the right camera images by adding the offset to the steering angle.\n",
    "    log_right = log[['right_img', 'steering_angle']].copy()\n",
    "    log_right.loc[:, 'steering_angle'] += SHIFT_OFFSET\n",
    "\n",
    "    paths = pd.concat([log.center_img, log_left.left_img, log_right.right_img]).str.strip()\n",
    "    values = pd.concat([log.steering_angle, log_left.steering_angle, log_right.steering_angle])\n",
    "\n",
    "    datagen = RegressionImageDataGenerator(rescale=lambda x: x / 127.5 - 1.,\n",
    "                                           horizontal_flip=True,\n",
    "                                           channel_shift_range=0.2,\n",
    "                                           width_shift_range=SHIFT_RANGE,\n",
    "                                           width_shift_value_transform=lambda val, shift: val - (\n",
    "                                               (SHIFT_OFFSET / SHIFT_RANGE) * shift),\n",
    "                                           horizontal_flip_value_transform=lambda val: -val,\n",
    "                                           cropping=CROPPING)\n",
    "\n",
    "    val_datagen = RegressionImageDataGenerator(rescale=lambda x: x / 127.5 - 1.,\n",
    "                                               cropping=CROPPING)\n",
    "\n",
    "    if fit_sample_size is not None:\n",
    "        sample_to_fit = load_images(paths.sample(fit_sample_size), IMG_SIZE)\n",
    "        datagen.fit(sample_to_fit)\n",
    "        val_datagen.fit(sample_to_fit)\n",
    "        del sample_to_fit\n",
    "\n",
    "    if from_directory:\n",
    "        return (datagen.flow_from_directory(paths.values, values.values, shuffle=True, target_size=IMG_SIZE,\n",
    "                                            batch_size=batch_size),\n",
    "                val_datagen.flow_from_directory(val_log.center_img.values, val_log.steering_angle.values, shuffle=True,\n",
    "                                                target_size=IMG_SIZE, batch_size=batch_size))\n",
    "    else:\n",
    "        images = load_images(paths, IMG_SIZE)\n",
    "        val_images = load_images(val_log.center_img, IMG_SIZE)\n",
    "\n",
    "        return (datagen.flow(images, values.values, shuffle=True, batch_size=batch_size),\n",
    "                val_datagen.flow(val_images, val_log.steering_angle.values, shuffle=True, batch_size=batch_size))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_model():\n",
    "    input_layer = Input(shape=get_cropped_shape((*IMG_SIZE, 3), CROPPING))\n",
    "    base_model = VGG16(weights='imagenet', include_top=False, input_tensor=input_layer)\n",
    "\n",
    "    # Remove the last block of the VGG16 net.\n",
    "    [base_model.layers.pop() for _ in range(4)]\n",
    "    base_model.outputs = [base_model.layers[-1].output]\n",
    "    base_model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    # Make sure pre trained layers from the VGG net don't change while training.\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add last block to the VGG model with modified sub sampling.\n",
    "    layer = base_model.outputs[0]\n",
    "    layer = Convolution2D(512, 3, 3, subsample=(2, 2), activation='relu', border_mode='same', name='block5_conv1')(\n",
    "        layer)\n",
    "    layer = Convolution2D(512, 3, 3, subsample=(1, 2), activation='relu', border_mode='same', name='block5_conv2')(\n",
    "        layer)\n",
    "    layer = Convolution2D(512, 3, 3, subsample=(1, 2), activation='relu', border_mode='same', name='block5_conv3')(\n",
    "        layer)\n",
    "\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dropout(.2)(layer)\n",
    "    layer = Dense(2048, activation='relu', name='fc1')(layer)\n",
    "    layer = Dropout(.2)(layer)\n",
    "    layer = Dense(1024, activation='relu', name='fc2')(layer)\n",
    "    layer = Dropout(.5)(layer)\n",
    "    layer = Dense(1, activation='linear', name='predictions')(layer)\n",
    "\n",
    "    return Model(input=base_model.input, output=layer)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = get_model()\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model.png', show_shapes=True)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Persist trained model\n",
    "    model_json = model.to_json()\n",
    "    with open('model.json', 'w') as f:\n",
    "        json.dump(model_json, f)\n",
    "\n",
    "    rdi_train, rdi_val = get_generator(TRAINING_DATA_PATHS, VALIDATION_DATA_PATHS, True, batch_size=BATCH_SIZE)\n",
    "\n",
    "    checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1, save_best_only=True,\n",
    "                                 save_weights_only=False, mode='auto')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=PATIENCE, verbose=1, mode='auto')\n",
    "\n",
    "    # Train the model with exactly one version of each image\n",
    "    # what is rdi_train.N rdi_val.N? \n",
    "    history = model.fit_generator(rdi_train,\n",
    "                                  samples_per_epoch=rdi_train,\n",
    "                                  validation_data=rdi_val,\n",
    "                                  nb_val_samples=rdi_val,\n",
    "                                  nb_epoch=NB_EPOCH,\n",
    "                                  callbacks=[checkpoint, early_stopping])\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
